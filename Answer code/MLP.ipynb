{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dacon 제출.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rMFME9FuAaa"
      },
      "source": [
        "#### 파일 및 폴더 디렉토리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCeufuH7uAJ1"
      },
      "source": [
        "# PATH\n",
        "# ├── [This File (.py / .ipynb)]\n",
        "# ├── Data\n",
        "# │   ├── sample_submission.csv\n",
        "# │   ├── test_x.csv\n",
        "# │   └── train.csv\n",
        "# └── Result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtMdNcOMt2Ec"
      },
      "source": [
        "#### 라이브러리 Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKBUx7bVt-NG"
      },
      "source": [
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 재현성을 위한 Seed 고정\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDirhQe-uUFu"
      },
      "source": [
        "#### 데이터 전처리 및 변환\n",
        "1. 응답 시간('Q_E')과 사용하는 손('hand')은 영향이 없을 것으로 생각해 drop 했습니다.\n",
        "2. 가족 수('familysize')가 50 초과인 레코드는 train 데이터에서 drop 했습니다.\n",
        "3. 'Q_A' 및 'tp__'에 대한 응답 내용과 가족 수('familysize')는 float 타입을 이용했고 나머지의 변수에 대해서는 one-hot-encoding을 했습니다.\n",
        "4. float 타입의 변수는 적절히 scale/shift 하여 대부분의 값이 [-1, 1]의 범위에 오게 했습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b-B5kmquTwc"
      },
      "source": [
        "drop_list = ['QaE', 'QbE', 'QcE', 'QdE', 'QeE',\n",
        "             'QfE', 'QgE', 'QhE', 'QiE', 'QjE',\n",
        "             'QkE', 'QlE', 'QmE', 'QnE', 'QoE',\n",
        "             'QpE', 'QqE', 'QrE', 'QsE', 'QtE',\n",
        "             'index', 'hand']\n",
        "replace_dict = {'education': str, 'engnat': str, 'married': str, 'urban': str}\n",
        "\n",
        "train_data = pd.read_csv('./Data/train.csv')\n",
        "test_data = pd.read_csv('./Data/test_x.csv')\n",
        "train_data = train_data.drop(train_data[train_data.familysize > 50].index)\n",
        "train_y = train_data['voted']\n",
        "train_x = train_data.drop(drop_list + ['voted'], axis=1)\n",
        "test_x = test_data.drop(drop_list, axis=1)\n",
        "train_x = train_x.astype(replace_dict)\n",
        "test_x = test_x.astype(replace_dict)\n",
        "train_x = pd.get_dummies(train_x)\n",
        "test_x = pd.get_dummies(test_x)\n",
        "train_y = 2 - train_y.to_numpy()\n",
        "train_x = train_x.to_numpy()\n",
        "test_x = test_x.to_numpy()\n",
        "\n",
        "train_y_t = torch.tensor(train_y, dtype=torch.float32)\n",
        "train_x_t = torch.tensor(train_x, dtype=torch.float32)\n",
        "test_x_t = torch.tensor(test_x, dtype=torch.float32)\n",
        "train_x_t[:, :20] = (train_x_t[:, :20] - 3.) / 2.\n",
        "test_x_t[:, :20] = (test_x_t[:, :20] - 3.) / 2\n",
        "train_x_t[:, 20] = (train_x_t[:, 20] - 5.) / 4.\n",
        "test_x_t[:, 20] = (test_x_t[:, 20] - 5.) / 4.\n",
        "train_x_t[:, 21:31] = (train_x_t[:, 21:31] - 3.5) / 3.5\n",
        "test_x_t[:, 21:31] = (test_x_t[:, 21:31] - 3.5) / 3.5\n",
        "test_len = len(test_x_t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTU7ZVuNueNI"
      },
      "source": [
        "#### GPU 사용 및 학습 파라미터"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqlbMhMeueBU"
      },
      "source": [
        "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "N_REPEAT = 5\n",
        "N_SKFOLD = 10\n",
        "N_EPOCH = 48\n",
        "BATCH_SIZE = 72\n",
        "LOADER_PARAM = {\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'num_workers': 4,\n",
        "    'pin_memory': True\n",
        "}\n",
        "prediction = np.zeros((test_len, 1), dtype=np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ag5E7rxdyCI_"
      },
      "source": [
        "#### 학습\n",
        "1. K-Fold Cross Valiation을 적용했습니다. (10 Fold(s))\n",
        "2. 두 개의 hidden layer을 갖는 간단한 신경망 모델을 이용했습니다.\n",
        "3. 모델의 크기와 (미니) 배치 크기, lr, weight_decay 등의 하이퍼파라미터는 optuna를 이용해 최적화한 값입니다. \n",
        "4. 전체 K-Fold Cross Valiation의 과정을 5번 반복했습니다. \n",
        "5. Binary Cross Entropy를 loss 함수로 사용했고 label의 True/False 불균형을 해결하기 위해 True인 경우에 약 1.2의 가중치를 부여했습니다.\n",
        "6. 최종 결과는 확률 값을 산술 평균한 결과입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9fiw3uvtvie"
      },
      "source": [
        "for repeat in range(N_REPEAT):\n",
        "\n",
        "    skf, tot = StratifiedKFold(n_splits=N_SKFOLD, random_state=repeat, shuffle=True), 0.\n",
        "    for skfold, (train_idx, valid_idx) in enumerate(skf.split(train_x, train_y)):\n",
        "        train_idx, valid_idx = list(train_idx), list(valid_idx)\n",
        "        train_loader = DataLoader(TensorDataset(train_x_t[train_idx, :], train_y_t[train_idx]),\n",
        "                                  shuffle=True, drop_last=True, **LOADER_PARAM)\n",
        "        valid_loader = DataLoader(TensorDataset(train_x_t[valid_idx, :], train_y_t[valid_idx]),\n",
        "                                  shuffle=False, drop_last=False, **LOADER_PARAM)\n",
        "        test_loader = DataLoader(TensorDataset(test_x_t, torch.zeros((test_len,), dtype=torch.float32)),\n",
        "                                 shuffle=False, drop_last=False, **LOADER_PARAM)\n",
        "        model = nn.Sequential(\n",
        "            nn.Dropout(0.05),\n",
        "            nn.Linear(91, 180, bias=False),\n",
        "            nn.LeakyReLU(0.05, inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(180, 32, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(32, 1)\n",
        "        ).to(DEVICE)\n",
        "        criterion = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1.20665], device=DEVICE))\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=5e-3, weight_decay=7.8e-2)\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            optimizer, T_0=N_EPOCH // 6, eta_min=4e-4)\n",
        "        prediction_t, loss_t = np.zeros((test_len, 1), dtype=np.float32), 1.\n",
        "\n",
        "        for epoch in tqdm(range(N_EPOCH), desc='{:02d}/{:02d}'.format(skfold + 1, N_SKFOLD)):\n",
        "            model.train()\n",
        "            for idx, (xx, yy) in enumerate(train_loader):\n",
        "                optimizer.zero_grad()\n",
        "                xx, yy = xx.to(DEVICE), yy.to(DEVICE)\n",
        "                pred = model(xx).squeeze()\n",
        "                loss = criterion(pred, yy)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                scheduler.step(epoch + idx / len(train_loader))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                running_acc, running_loss, running_count = 0, 0., 0\n",
        "                for xx, yy in valid_loader:\n",
        "                    xx, yy = xx.to(DEVICE), yy.to(DEVICE)\n",
        "                    pred = model(xx).squeeze()\n",
        "                    loss = criterion(pred, yy)\n",
        "                    running_loss += loss.item() * len(yy)\n",
        "                    running_count += len(yy)\n",
        "                    running_acc += ((torch.sigmoid(pred) > 0.5).float() == yy).sum().item()\n",
        "\n",
        "                if running_loss / running_count < loss_t:\n",
        "                    loss_t = running_loss / running_count\n",
        "                    for idx, (xx, _) in enumerate(test_loader):\n",
        "                        xx = xx.to(DEVICE)\n",
        "                        pred = (2. - torch.sigmoid(model(xx).detach().to('cpu'))).numpy()\n",
        "                        prediction_t[BATCH_SIZE * idx:min(BATCH_SIZE * (idx + 1), len(prediction)), :] \\\n",
        "                            = pred[:, :].copy()\n",
        "        prediction[:, :] += prediction_t[:, :].copy() / (N_REPEAT * N_SKFOLD)\n",
        "        tot += loss_t\n",
        "    print('R{} -> {:6.4f}'.format(repeat + 1, tot / N_SKFOLD))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9agsTzGukRX"
      },
      "source": [
        "#### 결과 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maWKneUEujsN"
      },
      "source": [
        "df = pd.read_csv('./Data/sample_submission.csv')\n",
        "df.iloc[:, 1:] = prediction\n",
        "df.to_csv('./Result/{}.csv'.format(datetime.now().strftime('%m%d-%H%M')), index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}